[![ru](https://img.shields.io/badge/lang-ru-red.svg)](README.md)

# Toxic comments classification. 
Development of a model for categorizing comments into positive and negative comments<br>
<u><b>Attention! This project includes profanity analisys.</b></u>
[Project permalink.]()

## Steps:
<li>EDA. 
<li><ul>Preprocessing.
<li>Text filtering and lemmatization</li>
<li>Feature generation</li></ul>
<li>Models validation ant testing.
  
## Utilized libraries and tools:
<li>PyTorch.
<li>NLTK.
<li>Spacy.
<li>Detoxify.
<li>Bert, ToxicBert.
<li>Profanity dataset.
<li>Scikit-Learn: LogisticRegression, TfidfVectorizer, CountVectorizer. 
<li>CatBoostClassifier.
<li>Transformers.
<li>Pandas. 
<li>Matplotlib. 
<li>Seaborn. 
<li>Wordcloud.
<li>Numpy. 
<li>IO, OS, Requests.
