[![ru](https://img.shields.io/badge/lang-ru-red.svg)](README.md)

# Toxic comments classification. 
Development of a model for categorizing comments into positive and negative comments<br>
<u><b>Attention! This project includes profanity analisys.</b></u><br>
[Project permalink.](https://github.com/mrBrain101/Yandex_Practicum_projects/blob/252b54e221a78f82e25ea3cd6ba972843142b394/NLP_Toxic_Commentaries_Classification/Ya_Practicum-NLP_Text_Toxicity_Prediction_distr_RUS.ipynb)

## Steps:
<li>EDA. 
<li>Preprocessing.<ul>
<li>Text filtering and lemmatization</li>
<li>Feature generation</li></ul>
<li>Models validation ant testing.
  
## Utilized libraries and tools:
<li>PyTorch.
<li>NLTK.
<li>Spacy.
<li>Detoxify.
<li>Bert, ToxicBert.
<li>Profanity dataset.
<li>Scikit-Learn: LogisticRegression, TfidfVectorizer, CountVectorizer. 
<li>CatBoostClassifier.
<li>Transformers.
<li>Pandas. 
<li>Matplotlib. 
<li>Seaborn. 
<li>Wordcloud.
<li>Numpy. 
<li>IO, OS, Requests.
