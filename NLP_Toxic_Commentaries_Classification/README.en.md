[![ru](https://img.shields.io/badge/lang-ru-red.svg)](README.md)

# Toxic comments classification. 

## Problem.
Development of a model for categorizing comments into positive and negative comments.
## Solution.
A _toxic-bert_ comment toxicity assessment model was used to classify toxic comments and logistic regression was applied: _F1_= 0.93. 

<u><b>Attention! This project includes profanity analysis.</b></u><br>
[Project permalink.](https://github.com/mrBrain101/Yandex_Practicum_projects/blob/252b54e221a78f82e25ea3cd6ba972843142b394/NLP_Toxic_Commentaries_Classification/Ya_Practicum-NLP_Text_Toxicity_Prediction_distr_RUS.ipynb)

## Steps:
<li>EDA. 
<li>Preprocessing.
<ul>
<li>Text filtering and lemmatization</li>
<li>Feature generation</li>
</ul>
<li>Models validation ant testing.
  
## Utilized libraries and tools:
<li>PyTorch.
<li>NLTK.
<li>Spacy.
<li>Detoxify.
<li>Bert, ToxicBert.
<li>Profanity dataset.
<li>Scikit-Learn: LogisticRegression, TfidfVectorizer, CountVectorizer. 
<li>CatBoostClassifier.
<li>Transformers.
<li>Pandas. 
<li>Matplotlib. 
<li>Seaborn. 
<li>Wordcloud.
<li>Numpy. 
<li>IO, OS, Requests.
