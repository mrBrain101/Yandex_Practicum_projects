[![en](https://img.shields.io/badge/lang-en-red.svg)](README.en.md)

# Классификация токсичных комментариев. 
Разработка модели классификации комментариев на позитивные и негативные.<br>
<u><b>Внимание! В работе анализируется нецензурная лексика на английском языке.</b></u>
[Ссылка на проект.]()

## Краткое содержание:
<li>Исследовательский анализ данных. 
<li><ul>Предобработка данных:
<li>Фильтрация и лемматизация</li>
<li>Визуализация лемм</li>
<li>Создание признаков</li></ul>
<li>Валидация и тестирование моделей.
  
## Библиотеки и инструменты:
<li>PyTorch.
<li>NLTK.
<li>Spacy.
<li>Detoxify.
<li>Bert, ToxicBert.
<li>Scikit-Learn: LogisticRegression, TfidfVectorizer, CountVectorizer. 
<li>CatBoostClassifier.
<li>Transformers.
<li>Pandas. 
<li>Matplotlib. 
<li>Seaborn. 
<li>Wordcloud.
<li>Numpy. 
<li>IO, OS, Requests.
